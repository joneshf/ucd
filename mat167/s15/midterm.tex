\documentclass[12pt,letterpaper]{article}

\usepackage[margin=1in]{geometry}
\usepackage[round-mode=figures,round-precision=3,scientific-notation=false]{siunitx}
\usepackage[super]{nth}
\usepackage[title]{appendix}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cancel}
\usepackage{color, colortbl}
\usepackage{dcolumn}
\usepackage{enumitem}
\usepackage{fp}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{systeme}
\usepackage{tikz}
\usepackage{titling}

\usetikzlibrary{arrows, automata}

\usepgfplotslibrary{statistics}

\pgfplotsset{compat=1.8}

\definecolor{Gray}{gray}{0.8}

\newcolumntype{d}{D{.}{.}{-1}}
\newcolumntype{g}{>{\columncolor{Gray}}c}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand*\circled[1]{
  \tikz[baseline=(char.base)]{
    \node[shape=circle,draw,inner sep=2pt] (char) {#1};
  }
}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\renewcommand{\labelenumii}{(\arabic*)}

\setlength{\droptitle}{-10ex}

\preauthor{\begin{flushright}\large \lineskip 0.5em}
\postauthor{\par\end{flushright}}
\predate{\begin{flushright}\large}
\postdate{\par\end{flushright}}

\title{MAT 167 Midterm\vspace{-2ex}}
\author{Hardy Jones\\
        999397426\\
        Professor Cheer\vspace{-2ex}}
\date{Spring 2015}

\begin{document}
  \maketitle

  \begin{enumerate}
    \item
      \begin{enumerate}
        \item
          True.

          \begin{proof}
            Assume $A = L_1U_1 = L_2U_2$ with $L_1, L_2$ lower triangular and unit diagonal,
            $U_1, U_2$ upper triangular with nonzero diagonal.

            So we have inverses for $L_1, L_2$ since the diagonal is all 1 with 0's above the diagonal,
            and we have inverses for $U_1, U_2$ since the diagonal is non-zero with 0's below the diagonal.

            Then we have:

            \begin{align*}
              L_1U_1 &= L_2U_2 \\
              L_2^{-1}\left(L_1U_1\right) &= L_2^{-1}\left(L_2U_2\right) \\
              L_2^{-1}\left(L_1U_1\right) &= \left(L_2^{-1}L_2\right)U_2 \\
              L_2^{-1}\left(L_1U_1\right) &= IU_2 \\
              L_2^{-1}\left(L_1U_1\right) &= U_2 \\
              L_2^{-1}\left(L_1U_1\right)U_1^{-1} &= U_2U_1^{-1} \\
              L_2^{-1}L_1\left(U_1U_1^{-1}\right) &= U_2U_1^{-1} \\
              L_2^{-1}L_1I &= U_2U_1^{-1} \\
              L_2^{-1}L_1 &= U_2U_1^{-1} \\
            \end{align*}

            Now, $L_2^{-1}L_1$ is a lower triangular matrix and
            $U_2U_1^{-1}$ is an upper triangular matrix.
            In order for these two to be equal they have to both be lower triangular and upper triangular at the same time.
            The only matrices with this property are diagonal matrices.

            So, $L_2^{-1}L_1, U_2U_1^{-1}$ are diagonal matrices.
            And since $L_2^{-1}L_1 = U_2U_1^{-1}$ we must have the same entries on the diagonal.
            And since $L_1, L_2$ have all 1 on the diagonal, $L_2^{-1}L_1$ has all 1 on the diagonal.

            So $L_2^{-1}L_1 = I$.

            Then,
            \begin{align*}
              L_2^{-1}L_1 &= I \\
              L_2\left(L_2^{-1}L_1\right) &= L_2I \\
              \left(L_2L_2^{-1}\right)L_1 &= L_2 \\
              IL_1 &= L_2 \\
              L_1 &= L_2
            \end{align*}

            And since $L_2^{-1}L_1 = I = U_2U_1^{-1}$, we have

            \begin{align*}
              I &= U_2U_1^{-1} \\
              IU_1 &= \left(U_2U_1^{-1}\right)U_1 \\
              U_1 &= U_2\left(U_1^{-1}U_1\right) \\
              U_1 &= U_2I \\
              U_1 &= U_2 \\
            \end{align*}

            So we have shown,

            if $A = L_1U_1 = L_2U_2$ with $L_1, L_2$ lower triangular and unit diagonal,
            $U_1, U_2$ upper triangular with nonzero diagonal,

            then $L_1 = L_2, U_1 = U_2$.
          \end{proof}
        \item
          True.

          \begin{proof}
            Assume $A^2 + A = I$
            \[
              A\left(A + I\right) = A^2 + A = I = A^2 + A = \left(A + I\right)A
            \]
            So $A + I$ is a left and right inverse of $A$,
            then $A^{-1} = A + I$.
          \end{proof}
        \item
          False.

          Let $A = \begin{bmatrix}0 & 1 \\ 1 & 0 \\\end{bmatrix}$.

          Then all the diagonal entries of $A$ are zero, but $\begin{vmatrix}0 & 1 \\ 1 & 0 \\\end{vmatrix} = -1$,
          so $A$ is non-singular.
      \end{enumerate}
    \item
      \begin{enumerate}
        \item
          The shape depends on $A$.

          If $A = 0$, then the shape is a point.

          If $A$ has a row or column with all 0, then the shape is a line.

          In any other case, then the region stays as a parallelogram.
          It can stay a square, become a rectangle, or a some other parallelogram, but it is always necessarily a parallelogram.
        \item
          The region is a square when $A = \begin{bmatrix}a \cos(\theta) & -b \sin(\theta) \\ b \sin(\theta) & a \cos(\theta)\end{bmatrix}$

          for some $a, b \in \mathbb{R}, \theta \in [0, 2\pi)$, with not $a = b = 0$.
        \item
          The region is a line when $A$ has exactly one row or column with all 0.
      \end{enumerate}
    \item
      \begin{enumerate}
        \item We have that $rank(A) = rank(U) = 3$.
        \item
          The basis for
          $R(A) = \left\{\begin{bmatrix}0 \\ 0 \\ 1 \\ -3 \\ 2\end{bmatrix}^T, \begin{bmatrix}2 \\ -1 \\ 4 \\ 2 \\ -1\end{bmatrix}^T, \begin{bmatrix}2 \\ -1 \\ 5 \\ -1 \\ 5\end{bmatrix}^T\right\}$
        \item
          False.

          row 1 + 2 $\cdot$ row 2 = row 3
        \item
          The basis for
          $C(A) = \left\{\begin{bmatrix}0 \\ 2 \\ 4 \\ 2\end{bmatrix}, \begin{bmatrix}1 \\ 4 \\ 9 \\ 5\end{bmatrix}, \begin{bmatrix}2 \\ 1 \\ 4 \\ 5\end{bmatrix}\right\}$
        \item We have $dim(N(A^T)) = 4 - rank(A) = 4 - 3 = 1$
        \item
          We can use the $LU$ decomposition.

          $Ax = 0 \implies Lc = 0, Ux = c$

          So we have for $c = \begin{bmatrix}c_1 \\ c_2 \\ c_3 \\ c_4 \\\end{bmatrix}$:
          \begin{itemize}
            \item $c_1 = 0$
            \item $c_2 = 0$
            \item $c_1 + c_2 + c_3 = 0 \implies c_3 = 0$
            \item $2c_1 + c_2 + c_4 = 0 \implies c_4 = 0$
          \end{itemize}

          Which gives us for $x = \begin{bmatrix}x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\\end{bmatrix}$
          \begin{itemize}
            \item $2x_5 = 0 \implies x_5 = 0$
            \item $x_3 - 3 x_4 + 2 x_5 = 0 \implies x_3 = 3 x_4$
            \item $2 x_1 - x_2 + 4 x_3 + 2 x_4 + x_5 = 0 \implies 2 x_1 = x_2 - 4 x_3 - 2 x_4 \implies x_1 = \frac{1}{2}x_2 - 7 x_4$
          \end{itemize}

          So we end up with the general solution:

          \[
            x =
            \begin{bmatrix}
              \frac{1}{2} \\
              1 \\
              0 \\
              0 \\
              0 \\
            \end{bmatrix}
            s
            +
            \begin{bmatrix}
              -7 \\
              0 \\
              3 \\
              1 \\
              0 \\
            \end{bmatrix}
            t
          \]
          for any $s, t \in \mathbb{R}$.
      \end{enumerate}
    \item
      \begin{enumerate}
        \item
          We have $x_1 + 3x_2 - x_3 = 0 \implies x_3 = x_1 + 3x_2$,
          so
          \[
            x =
            \begin{bmatrix}
              1 \\
              0 \\
              1 \\
            \end{bmatrix}
            s
            +
            \begin{bmatrix}
              0 \\
              1 \\
              3 \\
            \end{bmatrix}
            t
          \]
          for any $s, t \in \mathbb{R}$.

          Then a basis for $V = \left\{\begin{bmatrix}1 \\ 0 \\ 1 \\\end{bmatrix}, \begin{bmatrix}0 \\ 1 \\ 3 \\\end{bmatrix}\right\}$

          And since $V^{\perp} = R(A)$,
          a basis for $V^{\perp} = \left\{\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix}\right\}$
        \item
          We need $\left\|\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix}\right\| = \sqrt{1 + 9 + 1} = \sqrt{11}$

          Then, an orthonormal basis for $V^{\perp} = \left\{\frac{1}{\sqrt{11}}\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix}\right\}$.

          Now we can find $P_1$ as:

          \begin{align*}
            P_1 &= \frac{\frac{1}{\sqrt{11}}\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix} \left(\frac{1}{\sqrt{11}}\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix}\right)^T}{\left(\frac{1}{\sqrt{11}}\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix}\right)^T \frac{1}{\sqrt{11}}\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix}} \\
            &= \frac{\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix} \left(\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix}\right)^T}{\left(\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix}\right)^T \begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix}} \\
            &= \frac{1}{11}\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix} \left(\begin{bmatrix}1 \\ 3 \\ -1 \\\end{bmatrix}\right)^T \\
            &= \frac{1}{11}\begin{bmatrix}1 & 3 & -1 \\ 3 & 9 & -3 \\ -1 & -3 & 1 \\\end{bmatrix} \\
          \end{align*}
        \item
          We take the basis of $V$ and construct a matrix $A = \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 3 \\\end{bmatrix}$.

          Now we can find $P_2$ as:

          \begin{align*}
            P_2 &= A\left(A^T A\right)^{-1} A^T \\
            &= \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 3 \\\end{bmatrix}\left(\begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 3 \\\end{bmatrix}^T \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 3 \\\end{bmatrix}\right)^{-1} \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 3 \\\end{bmatrix}^T \\
            &= \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 3 \\\end{bmatrix}\left(\begin{bmatrix}1 & 0 & 1 \\ 0 & 1 & 3 \\\end{bmatrix} \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 3 \\\end{bmatrix}\right)^{-1} \begin{bmatrix}1 & 0 & 1 \\ 0 & 1 & 3 \\\end{bmatrix} \\
            &= \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 3 \\\end{bmatrix} \begin{bmatrix}2 & 3 \\ 3 & 10 \\\end{bmatrix}^{-1} \begin{bmatrix}1 & 0 & 1 \\ 0 & 1 & 3 \\\end{bmatrix} \\
            &= \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 3 \\\end{bmatrix} \left(\frac{1}{20 - 9}\begin{bmatrix}10 & -3 \\ -3 & 2 \\\end{bmatrix}\right) \begin{bmatrix}1 & 0 & 1 \\ 0 & 1 & 3 \\\end{bmatrix} \\
            &= \frac{1}{11} \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 3 \\\end{bmatrix} \begin{bmatrix}10 & -3 \\ -3 & 2 \\\end{bmatrix} \begin{bmatrix}1 & 0 & 1 \\ 0 & 1 & 3 \\\end{bmatrix} \\
            &= \frac{1}{11} \begin{bmatrix}10 & -3 \\ -3 & 2 \\ 1 & 3 \\\end{bmatrix} \begin{bmatrix}1 & 0 & 1 \\ 0 & 1 & 3 \\\end{bmatrix} \\
            &= \frac{1}{11} \begin{bmatrix}10 & -3 & 1 \\ -3 & 2 & 3 \\ 1 & 3 & 10 \\\end{bmatrix} \\
          \end{align*}
      \end{enumerate}
    \item
      Let $E = \left\{A \in \mathbb{R}^{4 \times 4} | S^{-1}AS \text{ is diagonal} \right\}$
      \begin{enumerate}
        \item
          \begin{proof}
            We need to show that the zero vector is in $E$, and $E$ is closed under linear combinations.

            \begin{itemize}
              \item
                Choose $A = 0$.

                Then $S^{-1}0S = 0$, and since 0 is diagonal, $0 \in E$.
              \item
                Choose $c, d \in \mathbb{R}$, and $A, B \in \mathbb{R}$.

                Then $S^{-1}\left(cA + dB\right)S = \left(S^{-1}(cA) + S^{-1}(dB)\right)S = S^{-1}(cA)S + S^{-1}(dB)S = c\left(S^{-1}AS\right) + d\left(S^{-1}BS\right)$.

                Now, since $A, B$ are diagonal, and $S^{-1}AS, S^{-1}BS$ are diagonal,
                we know $c\left(S^{-1}AS\right), d\left(S^{-1}BS\right)$ are also diagonal,
                so their sum is diagonal as well.

                So $cA + dB \in E$.

                Note that we chose arbitrary $c, d \in \mathbb{R}$, and $A, B \in E$ so this holds for all cases,

                In particular the case where $d = 0, B = 0$,
                which gives a proof of closure under scalar multiplication.

                And also the case where $c = d = 1$,
                which gives a proof of closure under vector addition.
            \end{itemize}
          \end{proof}
        \item
          When $S = I, S^{-1} = I$.

          Then $E = \left\{A \in \mathbb{R}^{4 \times 4} | A \text{ is diagonal} \right\}$,
          which is the set of all 4 $\times$ 4 diagonal matrices.
        \item
          Since $S$ diagonalizes $4 \times 4$ matrices, $\Lambda$ must have 4 non-zero eigenvalues.

          Then the dimension must be 4.
      \end{enumerate}
  \end{enumerate}
\end{document}
